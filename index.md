---
layout: page
---
{% include JB/setup %}

<img style="float: right; width: 35%; padding: 6px;" src=" {{ site.url }}assets/sreyan_small.png">

I am Sreyan Ghosh, a 4th-year Computer Science Ph.D. student at the University of Maryland, College Park (UMD) and a student researcher at Nvidia. At UMD, I conduct my research in the [Gamma Lab](https://gamma.umd.edu/) under the mentorship of [Prof. Dinesh Manocha](https://www.cs.umd.edu/people/dmanocha) and [Prof. Ramani Duraiswami](https://www.cs.umd.edu/people/ramanid). At Nvidia, I work with the ADLR and Cosmos World Model team. My research focuses on advancing *multimodal intelligence*, with an emphasis on audioâ€”spanning speech, sounds, and music. I work on challenges such as building data- and compute-efficient audio models, improving audio representation learning, generating synthetic data, and enhancing perception and reasoning in AI systems. My research is proudly supported by the [NVIDIA Graduate Fellowship](http://go.umd.edu/Ghosh).    

I maintain a list of my publications and research implementations under the [Research]({{ site.url }}/research) tab. I am always open to collaborations, and please feel free to drop me a mail!    

[Google Scholar](https://scholar.google.com/citations?user=5HKZJHAAAAAJ&hl=en) | [CV]({{ site.url }}/assets/Sreyan_Ghosh_CV.pdf)  
Email: [gsreyan@gmail.com](mailto:gsreyan@gmail.com) ; [sreyang@umd.edu](mailto:sreyang@umd.edu)  

<!-- ðŸ“£ Excited to launch the [Audio Question Answering task at DCASE 2025](https://dcase.community/challenge2025/task-audio-question-answering), in collaboration with Adobe and NVIDIA. ðŸ“£   -->

<!-- #### I am always open to collaborations! Please fill out [this](https://docs.google.com/forms/d/1kQRJekonn8YglxIPH9OPcJCuI7NQK-E1wAywNAsSMoM/) form here and I would reach out if I have a project aligned with your interests. Thank You! -->

#### Updates

<div style="height:275px;overflow:auto;">
<table>
<col width="100px">
<col width="630px">
  <tr><td><b>Nov 2025:</b></td><td>We release  <a href="https://musicflamingo-nv-umd.github.io/" target="_blank">Music Flamingo</a>, an LALM with expert music understanding capabilities!</td></tr>
  <tr><td><b>Nov 2025:</b></td><td>MMAU-Pro accepted to AAAI 2026!</td></tr>
  <tr><td><b>Sep 2025:</b></td><td>Audio Flamingo 3 accepted to NeurIPS 2025 as a spotlight!</td></tr>
  <tr><td><b>Aug 2025:</b></td><td>We release <a href="https://arxiv.org/abs/2508.13992" target="_blank">MMAU-Pro</a>, a challenging and comprehensive benchmark for evaluating audio intelligence! More details under the <a href="https://sreyan88.github.io/research/" target="_blank">Research</a> section.</td></tr>
  <tr><td><b>July 2025:</b></td><td>We release <a href="https://arxiv.org/abs/2507.08128" target="_blank">Audio Flamingo 3</a>, the most open, capable and powerful large audio-language model yet! More details under the <a href="https://sreyan88.github.io/research/" target="_blank">Research</a> section.</td></tr>
  <tr><td><b>May 2025:</b></td><td>Failing Forward accepted to ACL 2025 (Findings)!</td></tr>
  <tr><td><b>May 2025:</b></td><td>Audio Flamingo 2 accepted to ICML 2025!</td></tr>
  <tr><td><b>Mar 2025:</b></td><td>We release <a href="https://arxiv.org/abs/2503.03983" target="_blank">Audio Flamingo 2</a>, a SOTA audio-language model outperforming most other frontier models on audio understanding and reasoning tasks. Check out the <a href="https://huggingface.co/spaces/nvidia/audio-flamingo-2" target="_blank">demo</a> here!</td></tr>
  <tr><td><b>Jan 2025:</b></td><td><a href="https://openreview.net/forum?id=3PRvlT8b1R" target="_blank">VDGD</a>, <a href="https://openreview.net/forum?id=TeVAZXr3yv" target="_blank">MMAU</a> (Spotlight) and <a href="https://openreview.net/forum?id=bR1J7SpzrD" target="_blank">Synthio</a> have been accepted to ICLR 2025! More details under the Research section.</td></tr>
  <tr><td><b>Jan 2025:</b></td><td><a href="https://arxiv.org/abs/2410.15062" target="_blank">PAT</a>, <a href="https://arxiv.org/pdf/2410.16505" target="_blank">RobustCLAP</a> and ProSE have been accepted to NAACL 2025! More details under the Research section.</td></tr>
    <tr><td><b>Dec 2024:</b></td><td><a href="https://arxiv.org/abs/2409.09213" target="_blank">ReCLAP</a> (and a total of 3 papers) have been accepted to ICASSP 2025! More details under the Research section.</td></tr>
  <tr><td><b>Dec 2024:</b></td><td>We are hosting the DCASE 2025 Task 5 in collaboration with NVIDIA! More details <a href="https://dcase.community/articles/challenge-tasks-for-dcase2025" target="_blank">here</a>.</td></tr>
  <tr><td><b>Nov 2024:</b></td><td>I was awarded the <a href="https://www.cs.umd.edu/article/2024/12/umd-cs-phd-student-receives-nvidia-graduate-fellowship%C2%A0" target="_blank">NVIDIA</a> and Apple graduate fellowships! I have decided to accept the NVIDIA fellowship.</td></tr>
  <tr><td><b>Sept 2024:</b></td><td>We released <a href="https://sakshi113.github.io/mmau_homepage/" target="_blank">MMAU</a>, the most comprehesive audio understanding and reasoning benchmark yet!</td></tr>
  <tr><td><b>Sept 2024:</b></td><td>2 papers accepted to EMNLP 2024 as oral presentations!</td></tr>
  <tr><td><b>Aug 2024:</b></td><td>Our workshop proposal, SALMA, has been accepted to ICASSP 2025!</td></tr>
  <tr><td><b>June 2024:</b></td><td>We release GAMA, an LLM with strong audio-understanding capabilities! Details under the Research section.</td></tr>
  <tr><td><b>May 2024:</b></td><td>1 paper accepted to InterSpeech 2024!</td></tr>
  <tr><td><b>May 2024:</b></td><td>Joined Microsoft in Redmond as a Research Scientist Intern!</td></tr>
  <tr><td><b>May 2024:</b></td><td>2 papers accepted to ACL 2024!</td></tr>
  <tr><td><b>May 2024:</b></td><td>1 paper accepted to ICML 2024!</td></tr>
  <tr><td><b>March 2024:</b></td><td>2 papers accepted to NAACL 2024!</td></tr>
  <tr><td><b>Feb 2024:</b></td><td>1 paper accepted to CVPR 2024!</td></tr>
  <tr><td><b>Jan 2024:</b></td><td>1 paper accepted to ICLR 2024!</td></tr>
  <tr><td><b>Dec 2023:</b></td><td>Awarded the UMD graduate school's Outstanding RA Award!</td></tr>
  <tr><td><b>Dec 2023:</b></td><td>3 papers accepted to ICASSP 2024! Details under the research section.</td></tr>
  <tr><td><b>Dec 2023:</b></td><td>Attended EMNLP 2023 in-person in Singapore!</td></tr>
  <tr><td><b>Oct 2023:</b></td><td>2 papers accepted to EMNLP 2023! Details under the research section.</td></tr>
  <tr><td><b>Oct 2023:</b></td><td>Attended ICCV 2023 in-person in Paris!</td></tr>
  <tr><td><b>Oct 2023:</b></td><td>Attended InterSpeech 2023 in-person in Dublin!</td></tr>
  <tr><td><b>May 2023:</b></td><td>Our paper was accepted to ICCV 2023!</td></tr>
  <tr><td><b>May 2023:</b></td><td>Started as a Research Scientist Intern at Adobe Research!</td></tr>
  <tr><td><b>May 2023:</b></td><td>Our paper was accepted to Interspeech 2023!</td></tr>
  <tr><td><b>Apr 2023:</b></td><td>Our paper was accepted to ACL 2023!</td></tr>
  <tr><td><b>Apr 2023:</b></td><td>Our paper was accepted to SIGIR 2023!</td></tr>
  <tr><td><b>Mar 2023:</b></td><td>Serving as a reviewer for Interspeech 2023!</td></tr>
  <tr><td><b>Feb 2023:</b></td><td>I got admitted to the C.S. Ph.D. program at UMD! I will be starting in the Fall of 2023!.</td></tr>
  <tr><td><b>Feb 2023:</b></td><td>3 papers accepted to ICASSP 2023! Pre-prints under the research section.</td></tr>
  <tr><td><b>Feb 2023:</b></td><td>Serving as a reviewer for ACL 2023!</td></tr>
  <tr><td><b>Jan 2023:</b></td><td>Submitted one paper to ACL 2023!</td></tr>
  <tr><td><b>Jan 2023:</b></td><td>Our team <em>Shravan</em> won the <em>Best Demo Implementation award</em> at the 2022 IEEE-SLT Code Hackathon! Links to slides and recording of the presentation to be posted soon under the Others tab.</td></tr>
  <tr><td><b>Jan 2023:</b></td><td>Served as a reviewer for AAAI 2023 Muffin Workshop.</td></tr>
  <tr><td><b>Dec 2022:</b></td><td>Served as a reviewer for ICASSP 2023.</td></tr>
  <tr><td><b>Nov 2022:</b></td><td>Served as a reviewer for AAAI 2023.</td></tr>
  <tr><td><b>Oct 2022:</b></td><td>4 papers submitted to IEEE ICASSP 2023! Pre-print and codes to be made available soon!</td></tr>
  <tr><td><b>Sept 2022:</b></td><td>2 papers accepted to IEEE SLT 2022! Pre-print and code now available!</td></tr>
  <tr><td><b>Aug 2022:</b></td><td>Paper on low-resource audio representation learning accepted to IEEE JSTSP Special Issue! More details under the research section!</td></tr>
  <tr><td><b>Aug 2022:</b></td><td>Moved to the beautiful city of College Park and started school at the University of Maryland!</td></tr>
  <tr><td><b>July 2022:</b></td><td>Started contributing to GSoC 2022 for the Keras Organization. More details about my project can be found in the Projects section!</td></tr>
  <tr><td><b>July 2022:</b></td><td>2 papers accepted to Interspeech 2022! Pre-print and codes now available now!</td></tr>  
  <tr><td><b>Dec 2021:</b></td><td>Paper on Low-Resource Audio Representation Learning accepted to AAAI 2022 SAS Workshop! Pre-print now available under research section!</td></tr>